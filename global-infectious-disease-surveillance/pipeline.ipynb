{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7207bce-2f0f-4c1d-8d55-fbbd287bd0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pipeline for Global Infectious Disease Surveillance project.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from config import RAW_DATA_DIR, PROCESSED_DATA_DIR, WHO_API_KEY\n",
    "\n",
    "\n",
    "WHO_BASE = \"https://ghoapi.azureedge.net/api\"\n",
    "\n",
    "\n",
    "def fetch_who_data(indicator: str = \"WHS9_86\") -> Path:\n",
    "    \"\"\"\n",
    "    Fetch WHO data for a given indicator and save to CSV.\n",
    "\n",
    "    indicator: WHO GHO code (placeholder used here).\n",
    "    \"\"\"\n",
    "    RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    url = f\"{WHO_BASE}/{indicator}\"\n",
    "    resp = requests.get(url, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()[\"value\"]\n",
    "    df = pd.DataFrame(data)\n",
    "    out_path = RAW_DATA_DIR / f\"who_{indicator}.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def fetch_ecdc_data() -> Path:\n",
    "    \"\"\"\n",
    "    TODO: Implement ECDC fetch.\n",
    "    For now, manually download latest ECDC outbreak table to data/raw and return path.\n",
    "    \"\"\"\n",
    "    # Placeholder: user manually saves file\n",
    "    return RAW_DATA_DIR / \"ecdc_outbreaks.csv\"\n",
    "\n",
    "\n",
    "def run_pipeline():\n",
    "    \"\"\"\n",
    "    Outline of analysis:\n",
    "\n",
    "    1. Fetch WHO & ECDC data.\n",
    "    2. Clean/normalize country codes and dates.\n",
    "    3. Engineer outbreak metrics (incidence, growth rates).\n",
    "    4. Fit forecasting model (Prophet) for key diseases/regions.\n",
    "    5. Run anomaly detection on forecast residuals.\n",
    "    6. Apply NLP keyword extraction on outbreak descriptions.\n",
    "    7. Export processed datasets for Tableau (wide and long).\n",
    "    \"\"\"\n",
    "    who_path = fetch_who_data()\n",
    "    ecdc_path = fetch_ecdc_data()\n",
    "\n",
    "    # 2) Cleaning + feature engineering (TODO: implement)\n",
    "    # df_who = clean_who(who_path)\n",
    "    # df_ecdc = clean_ecdc(ecdc_path)\n",
    "    # combined = combine_sources(df_who, df_ecdc)\n",
    "    # features = build_outbreak_features(combined)\n",
    "\n",
    "    # 3) Forecasting (Prophet) (TODO: implement)\n",
    "    # models, forecast_df = fit_prophet_models(features)\n",
    "\n",
    "    # 4) Anomaly detection (TODO)\n",
    "    # anomalies = detect_anomalies(forecast_df)\n",
    "\n",
    "    # 5) NLP (KeyBERT) on narrative fields (TODO)\n",
    "    # keywords = extract_keywords(combined[\"narrative\"])\n",
    "\n",
    "    # 6) Export for Tableau\n",
    "    PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    # features.to_csv(PROCESSED_DATA_DIR / \"outbreak_features.csv\", index=False)\n",
    "    # anomalies.to_csv(PROCESSED_DATA_DIR / \"outbreak_anomalies.csv\", index=False)\n",
    "    # keywords.to_csv(PROCESSED_DATA_DIR / \"outbreak_keywords.csv\", index=False)\n",
    "\n",
    "    print(\"Pipeline outline executed (fill in TODOs to make it live).\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
